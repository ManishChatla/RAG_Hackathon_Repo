import pandas as pd
import numpy as np
import joblib
import json
import time
from openai import OpenAI
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, r2_score
import os

# Initialize API and model paths
client = OpenAI(api_key="YOUR_API_KEY")
LABEL_FILE = "llm_labeled_subset.csv"
MODEL_FILE = "sentiment_model.pkl"
VECTORIZER_FILE = "tfidf_vectorizer.pkl"

# Load all complaints
df_all = pd.read_csv("complaints.csv")

# If labeled file doesnâ€™t exist, initialize it
if not os.path.exists(LABEL_FILE):
    labeled_df = pd.DataFrame(columns=["complaint_text", "response_text", "sentiment_score"])
else:
    labeled_df = pd.read_csv(LABEL_FILE)

# Identify unlabeled records
merged_df = df_all.merge(labeled_df, on=["complaint_text", "response_text"], how="left", indicator=True)
unlabeled_df = merged_df[merged_df["_merge"] == "left_only"][["complaint_text", "response_text"]]

print(f"Found {len(unlabeled_df)} new unlabeled records.")

# Label a small random batch with LLM (e.g., 100 samples)
new_batch = unlabeled_df.sample(min(100, len(unlabeled_df)), random_state=42).copy()

def llm_label(complaint, response):
    prompt = f"""
You are an expert in customer sentiment analysis.

Compare the complaint and the company's response.
Rate the overall sentiment from 1 to 5 (1 = very negative, 5 = very positive).
Return JSON only as: {{"score": number_between_1_and_5}}

Complaint: "{complaint}"
Response: "{response}"
"""
    try:
        completion = client.chat.completions.create(
            model="gpt-5",
            messages=[{"role": "user", "content": prompt}],
            temperature=0
        )
        result = completion.choices[0].message.content
        result_json = json.loads(result)
        return result_json["score"]
    except Exception as e:
        print("Error:", e)
        return None

print("\nLabeling new records with LLM...")

for i, row in new_batch.iterrows():
    new_batch.loc[i, "sentiment_score"] = llm_label(row["complaint_text"], row["response_text"])
    time.sleep(1.5)  # Prevent rate limit issues

# Append to labeled dataset and remove duplicates
labeled_df = pd.concat([labeled_df, new_batch]).drop_duplicates(subset=["complaint_text", "response_text"])
labeled_df.to_csv(LABEL_FILE, index=False)

print(f"âœ… Added {len(new_batch)} new labeled records. Total labeled now: {len(labeled_df)}")

# === Retrain ML model ===
print("\nRetraining ML model...")

# Prepare text features
labeled_df["combined_text"] = labeled_df["complaint_text"] + " " + labeled_df["response_text"]

vectorizer = TfidfVectorizer(stop_words="english", max_features=5000)
X = vectorizer.fit_transform(labeled_df["combined_text"])
y = labeled_df["sentiment_score"]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

model = LogisticRegression(max_iter=200)
model.fit(X_train, y_train)

# Evaluate
y_pred = model.predict(X_test)
print("Mean Absolute Error:", mean_absolute_error(y_test, y_pred))
print("RÂ² Score:", r2_score(y_test, y_pred))

# Save updated model and vectorizer
joblib.dump(model, MODEL_FILE)
joblib.dump(vectorizer, VECTORIZER_FILE)

print("\nâœ… Retraining complete. Model updated and saved.")

Use a Large Language Model (LLM) to auto-label a small portion of the dataset â†’ then train a traditional ML model (like Logistic Regression, SVM, or even BERT) on those labels â†’ and finally apply that ML model on the entire dataset (3K+ records) efficiently.

Full LLM â†’ ML â†’ Auto-Retrain Pipeline

ðŸ”¹ Stage 1: Initial LLM Labeling

Use GPT (or another LLM) to label the first few hundred complaintâ€“response pairs (as shown earlier).

ðŸ”¹ Stage 2: Train ML Model on LLM Labels

Train a model (e.g., Logistic Regression, SVM, or BERT) to learn sentiment scoring.

ðŸ”¹ Stage 3: Predict Remaining Data

Use the model to infer sentiment for unlabeled data.

ðŸ”¹ Stage 4: Continuous Improvement

When you add new records (or periodically every week/month):
	1.	Select a small random batch of new unlabeled data.
	2.	Label them using the LLM.
	3.	Merge with existing training data.
	4.	Retrain the ML model automatically.


