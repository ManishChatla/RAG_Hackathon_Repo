# ğŸš€ RAG Hackathon Project

This project implements an **Advanced Retrieval-Augmented Generation (RAG)** pipeline powered by **Gemini LLM** and **ChromaDB**. It supports both CLI-based execution and an interactive Streamlit UI for experimentation, visualization, and evaluation.

---

## ğŸ§© Project Overview

This repository demonstrates an **end-to-end RAG framework** with modular components, including:

- **Preprocessing & Chunking**
- **Embedding Generation (Gemini)**
- **Hybrid Retrieval (TF-IDF, BM25, ChromaDB)**
- **Reranking using LLM, Cohere, and CrossEncoder**
- **Confidence and Latency Evaluation**
- **Interactive Streamlit Interface**

---

## ğŸ“‚ Folder Structure

```
RAG_Hackathon_Repo/
â”‚
â”œâ”€â”€ code/
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â”‚   â”œâ”€â”€ input_data.json         # Input corpus for processing
â”‚   â”‚   â”‚   â”œâ”€â”€ chroma_db/              # ChromaDB vector store
â”‚   â”‚   â”œâ”€â”€ main.py                     # CLI-based execution entry point
â”‚   â”‚   â”œâ”€â”€ streamline_app.py           # Streamlit UI for interactive testing
â”‚   â”‚   â”œâ”€â”€ preprocess.py               # Text loading, cleaning, and chunking logic
â”‚   â”‚   â”œâ”€â”€ embedder.py                 # Gemini embedding generation
â”‚   â”‚   â”œâ”€â”€ vector_store.py             # ChromaDB integration for storage & retrieval
â”‚   â”‚   â”œâ”€â”€ reranker.py                 # Reranking methods (LLM, Cohere, CrossEncoder)
â”‚   â”‚   â”œâ”€â”€ config.py                   # Configuration constants
â”‚   â”‚   â””â”€â”€ prompts.py                  # RAG-specific prompt formatting
â”‚   â”‚
â”‚   â”œâ”€â”€ tests/                          # Unit tests for all modules
â”‚   â”‚   â”œâ”€â”€ test_chunking.py
â”‚   â”‚   â”œâ”€â”€ test_embedding.py
â”‚   â”‚   â”œâ”€â”€ test_vectorstore.py
â”‚   â”‚   â”œâ”€â”€ test_reranker.py
â”‚   â”‚   â””â”€â”€ test_main.py
â”‚   â”‚
â”‚   â””â”€â”€ requirements.txt
â”‚
â””â”€â”€ README.md
```

---

## âš™ï¸ Running the Project

### â–¶ï¸ Option 1: Command-line Execution
```bash
cd code/src
python main.py
```

### ğŸ’» Option 2: Streamlit UI
```bash
cd code/src
streamlit run streamline_app.py
```

---

## ğŸ§  Chunking Strategy

The text corpus is segmented into **semantic chunks** using a **hybrid chunking algorithm** that combines:

- **Sentence boundary detection**
- **Token-based windowing (e.g., 512â€“1024 tokens)**
- **Overlap context preservation (typically 10â€“20%)**

This ensures both **context continuity** and **retrieval efficiency**.

---

## ğŸ” Embedding Generation

We use **Geminiâ€™s embedding model** (`models/embedding-001`) to generate high-dimensional vectors. These vectors are stored in **ChromaDB**, which supports efficient retrieval using **HNSW indexing** (Hierarchical Navigable Small World graphs).

---

## ğŸ§® Hybrid Retrieval Approach

The retrieval pipeline combines **semantic similarity** and **lexical relevance** using:

- **TF-IDF** â€“ For fast keyword relevance scoring  
- **BM25** â€“ For improved term weighting and ranking  
- **ChromaDB (Vector-based)** â€“ For semantic retrieval using cosine similarity  
- **Query Expansion** â€“ For reformulating user queries to enhance recall

The final retrieval results are **merged and deduplicated** for better coverage.

---

## ğŸ” Reranking Techniques

To improve contextual accuracy, retrieved documents are reranked using **three distinct approaches**:

1. **LLM-based Reranker** â€“ Uses Gemini to analyze contextual match with the query.  
2. **Cohere Reranker** â€“ Leverages Cohereâ€™s `rerank-english-v2.0` model for relevance scoring.  
3. **CrossEncoder Reranker** â€“ Uses transformer-based pair scoring (query, document) similarity.

The combined reranking score enhances the precision of the final retrieval set.

---

## ğŸ’¾ Vector Storage (ChromaDB)

We use **ChromaDB** for efficient similarity search. It provides:

- Persistent **vector store**
- Optimized **HNSW graph indexing**
- Metadata-based **filtering & retrieval**

Each chunk is indexed with metadata like `doc_id`, `chunk_id`, and `source_file`.

---

## ğŸ“Š Evaluation Metrics

During retrieval and reranking, the following metrics are calculated:

| Metric | Description |
|--------|--------------|
| **Retrieval Confidence** | Average cosine similarity of top matches |
| **Answer Confidence** | LLM-based estimate of response certainty |
| **Token Count** | Number of tokens used in generation |
| **Retrieval Latency** | Time taken for embedding & retrieval |

---

## âœ… Testing & Coverage

All major modules have Pytest-based unit tests.  
To run tests with coverage:

```bash
pytest --maxfail=1 --disable-warnings -q
pytest --cov=src --cov-report=term-missing
```

---

## ğŸ§  Summary

This RAG framework brings together **semantic, lexical, and contextual intelligence**. By integrating **Gemini embeddings**, **hybrid retrieval**, and **multi-model reranking**, it achieves a **balanced blend of recall and precision**â€”making it ideal for enterprise-scale retrieval applications.

---

**Authors:** Manish & Team  
**Hackathon:** Advanced RAG Challenge  
**Tech Stack:** Python, Streamlit, ChromaDB, Gemini, Cohere, HuggingFace Transformers
